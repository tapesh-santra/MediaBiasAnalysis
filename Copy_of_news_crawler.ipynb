{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of news_crawler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tapesh-santra/MediaBiasAnalysis/blob/master/Copy_of_news_crawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQfmMM_AwDdX",
        "colab_type": "code",
        "outputId": "921fba36-f999-4e17-f820-7a9c448f22dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aVdVPpqUVSA",
        "colab_type": "text"
      },
      "source": [
        "# First we shall train a model to identify unreliable news articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRSUWHH-VC2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tau9FSWIUUlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load training and test data\n",
        "project_dir='/content/drive/My Drive/news_crawler/fake_news_data/'\n",
        "train=pd.read_csv(project_dir+'train.csv')\n",
        "test=pd.read_csv(project_dir+'test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvtL7M8SY-CR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare data for feature engineering\n",
        "test=test.fillna(' ')\n",
        "train=train.fillna(' ')\n",
        "\n",
        "# create n-gram features\n",
        "count_vectorizer = CountVectorizer(ngram_range=(1, 2),min_df=0.01,max_df=0.7, stop_words='english')\n",
        "counts = count_vectorizer.fit_transform(train['text'].values)\n",
        "\n",
        "# IDF normalized term frequencies\n",
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "tfidf = transformer.fit_transform(counts)\n",
        "\n",
        "#Extract feature from test data\n",
        "test_counts = count_vectorizer.transform(test['text'].values)\n",
        "test_tfidf = transformer.fit_transform(test_counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XutDnzBo8s1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train a model\n",
        "targets = train['label'].values\n",
        "\n",
        "#split in samples\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf, targets, random_state=0)\n",
        "\n",
        "#Fit models\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvwZvNhpC2f-",
        "colab_type": "code",
        "outputId": "c7d3d30c-4de5-473f-d844-7b647023c4ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n",
        "                              AdaBoostClassifier)\n",
        "#Extra Tree Classifier\n",
        "Extr = ExtraTreesClassifier(n_estimators=25,n_jobs=4)\n",
        "Extr.fit(X_train, y_train)\n",
        "print('Accuracy of ExtrTrees classifier on training set: {:.2f}'\n",
        "     .format(Extr.score(X_train, y_train)))\n",
        "print('Accuracy of Extratrees classifier on test set: {:.2f}'\n",
        "     .format(Extr.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of ExtrTrees classifier on training set: 1.00\n",
            "Accuracy of Extratrees classifier on test set: 0.92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4TBmfDGME9v",
        "colab_type": "code",
        "outputId": "681dc6fd-0117-4de7-e5fc-90ad8afa3d07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "gbc=GradientBoostingClassifier(learning_rate=0.1,n_estimators=50)\n",
        "gbc.fit(X_train,y_train)\n",
        "print('Accuracy of GradientBoosting classifier on training set: {:.2f}'\n",
        "     .format(gbc.score(X_train, y_train)))\n",
        "print('Accuracy of GradientBoosting classifier on test set: {:.2f}'\n",
        "     .format(gbc.score(X_test, y_test)))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of GradientBoosting classifier on training set: 0.92\n",
            "Accuracy of GradientBoosting classifier on test set: 0.92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNFCyoXsDK-z",
        "colab_type": "code",
        "outputId": "a59744c5-43f4-4264-cc36-13d70e2551c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Adaboost classifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "Adab= AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),n_estimators=25)\n",
        "Adab.fit(X_train, y_train)\n",
        "print('Accuracy of Adaboost classifier on training set: {:.2f}'\n",
        "     .format(Adab.score(X_train, y_train)))\n",
        "print('Accuracy of Adaboost classifier on test set: {:.2f}'\n",
        "     .format(Adab.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Adaboost classifier on training set: 0.96\n",
            "Accuracy of Adaboost classifier on test set: 0.93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uKpXX82Dd7k",
        "colab_type": "code",
        "outputId": "dde11ad6-fac4-4428-8240-20dd6314e837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "Rando= RandomForestClassifier(n_estimators=25)\n",
        "\n",
        "Rando.fit(X_train, y_train)\n",
        "print('Accuracy of randomforest classifier on training set: {:.2f}'\n",
        "     .format(Rando.score(X_train, y_train)))\n",
        "print('Accuracy of randomforest classifier on test set: {:.2f}'\n",
        "     .format(Rando.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of randomforest classifier on training set: 1.00\n",
            "Accuracy of randomforest classifier on test set: 0.93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMJCQ3_-DrcA",
        "colab_type": "code",
        "outputId": "c7386d16-261a-4a8e-c342-e9ccba4a1997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Simple logistic regression model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(C=1e5)\n",
        "logreg.fit(X_train, y_train)\n",
        "print('Accuracy of Lasso classifier on training set: {:.2f}'\n",
        "     .format(logreg.score(X_train, y_train)))\n",
        "print('Accuracy of Lasso classifier on test set: {:.2f}'\n",
        "     .format(logreg.score(X_test, y_test)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of Lasso classifier on training set: 1.00\n",
            "Accuracy of Lasso classifier on test set: 0.94\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vqy0UE61D_Dd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Save the model to the disk\n",
        "import pickle\n",
        "project_dir='/content/drive/My Drive/news_crawler/fake_news_data/'\n",
        "pickle.dump(logreg,open(project_dir+'logreg.pkl','wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsXsIW9PxhCE",
        "colab_type": "text"
      },
      "source": [
        "# Time to scrape some data from \"News\" Channels\n",
        "## Let's start with CNN\n",
        "Install some useful libraries first "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeOiFHCRUTp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install beautifulsoup4\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1gNv8enx2D7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install newspaper3k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeqT7RGExrdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install feedparser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wS_YzEyfSO99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the model\n",
        "import pickle\n",
        "clf=pickle.load(open(project_dir+'logreg.pkl','rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unwje33dw_1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import feedparser as fp\n",
        "import json\n",
        "import newspaper\n",
        "from newspaper import Article\n",
        "from bs4 import BeautifulSoup\n",
        "from time import mktime\n",
        "from datetime import datetime\n",
        "import urllib.request\n",
        "import requests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc4w2vK2_RrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We shall download links from google news api (it's depricated but still works)\n",
        "#First fox news\n",
        "fox_news=[]\n",
        "for page in range(1,6):\n",
        "  url='https://newsapi.org/v2/everything?q=politics&sources=fox-news&page='+str(page)+'&sortBy=publishedAt&apiKey=f78d1be7057240008ba795b57929a5b9'\n",
        "  data=json.loads(requests.get(url).text)\n",
        "  if data['status']=='ok':\n",
        "    for j in range(20):\n",
        "      url=data['articles'][j]['url']\n",
        "      article=Article(url)\n",
        "      article.download()\n",
        "      article.parse()\n",
        "      fox_news.append(article.text)\n",
        "      #print('...................................')\n",
        "  else:\n",
        "    print(data['message'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ExzHBkaIMIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cnn news\n",
        "cnn_news=[]\n",
        "for page in range(1,6):\n",
        "  url='https://newsapi.org/v2/everything?q=politics&sources=cnn&page='+str(page)+'&sortBy=publishedAt&apiKey=f78d1be7057240008ba795b57929a5b9'\n",
        "  data=json.loads(requests.get(url).text)\n",
        "  if data['status']=='ok':\n",
        "    for j in range(20):\n",
        "      url=data['articles'][j]['url']\n",
        "      article=Article(url)\n",
        "      article.download()\n",
        "      article.parse()\n",
        "      cnn_news.append(article.text)\n",
        "      #print('...................................')\n",
        "  else:\n",
        "    print(data['message'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSQXgV7iJCW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#bbc news\n",
        "bbc_news=[]\n",
        "for page in range(1,6):\n",
        "  url='https://newsapi.org/v2/everything?q=politics&sources=bbc-news&page='+str(page)+'&sortBy=publishedAt&apiKey=f78d1be7057240008ba795b57929a5b9'\n",
        "  data=json.loads(requests.get(url).text)\n",
        "  if data['status']=='ok':\n",
        "    for j in range(20):\n",
        "      url=data['articles'][j]['url']\n",
        "      article=Article(url)\n",
        "      article.download()\n",
        "      article.parse()\n",
        "      bbc_news.append(article.text)\n",
        "      #print('...................................')\n",
        "  else:\n",
        "    print(data['message'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHTeOw2XKIOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import pandas as pd\n",
        "#import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "#predict for cnn\n",
        "#Extract feature from test data\n",
        "cnn_counts = count_vectorizer.transform(np.array(cnn_news))\n",
        "cnn_tfidf = transformer.fit_transform(cnn_counts)\n",
        "\n",
        "#Now run the model on data\n",
        "p_cnn=np.zeros((100,5))\n",
        "p_cnn[:,0]=logreg.predict(cnn_tfidf)\n",
        "p_cnn[:,1]=Extr.predict(cnn_tfidf)\n",
        "p_cnn[:,2]=gbc.predict(cnn_tfidf)\n",
        "p_cnn[:,3]=Adab.predict(cnn_tfidf)\n",
        "p_cnn[:,4]=Rando.predict(cnn_tfidf)\n",
        "cnn_reliability=np.sum(p_cnn,axis=1)\n",
        "\n",
        "\n",
        "#######################\n",
        "#fox news\n",
        "fox_counts = count_vectorizer.transform(np.array(fox_news))\n",
        "fox_tfidf = transformer.fit_transform(fox_counts)\n",
        "\n",
        "#Now run the model on data\n",
        "p_fox=np.zeros((100,5))\n",
        "p_fox[:,0]=logreg.predict(fox_tfidf)\n",
        "p_fox[:,1]=Extr.predict(fox_tfidf)\n",
        "p_fox[:,2]=gbc.predict(fox_tfidf)\n",
        "p_fox[:,3]=Adab.predict(fox_tfidf)\n",
        "p_fox[:,4]=Rando.predict(fox_tfidf)\n",
        "fox_reliability=np.sum(p_fox,axis=1)\n",
        "\n",
        "\n",
        "#bbc news\n",
        "bbc_counts = count_vectorizer.transform(np.array(bbc_news))\n",
        "bbc_tfidf = transformer.fit_transform(bbc_counts)\n",
        "\n",
        "#Now run the model on data\n",
        "p_bbc=np.zeros((100,5))\n",
        "p_bbc[:,0]=logreg.predict(bbc_tfidf)\n",
        "p_bbc[:,1]=Extr.predict(bbc_tfidf)\n",
        "p_bbc[:,2]=gbc.predict(bbc_tfidf)\n",
        "p_bbc[:,3]=Adab.predict(bbc_tfidf)\n",
        "p_bbc[:,4]=Rando.predict(bbc_tfidf)\n",
        "bbc_reliability=np.sum(p_bbc,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-StyUHqdPHfc",
        "colab_type": "code",
        "outputId": "06d3096a-471d-48c2-e65f-44b42ae50969",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "from collections import Counter\n",
        "cnn=Counter(cnn_reliability)\n",
        "fox=Counter(fox_reliability)\n",
        "bbc=Counter(bbc_reliability)\n",
        "#plt.figure();\n",
        "#plt.bar([0,1,2,3,4,5],np.fliplr(cnn.values()))\n",
        "#plt.bar([0,1,2,3,4,5],np.fliplr(fox.values()))\n",
        "sorted_cnn = [cnn[k] for k in sorted(cnn)]\n",
        "sorted_fox=  [fox[k] for k in sorted(fox)]\n",
        "sorted_bbc=  [bbc[k] for k in sorted(bbc)]\n",
        "\n",
        "ind = np.arange(6)  # the x locations for the groups\n",
        "width = 0.28       # the width of the bars\n",
        "\n",
        "fig = plt.figure(figsize=(12,5))\n",
        "ax = fig.add_subplot(121)\n",
        "rects1 = ax.bar(ind, sorted_cnn, width, color='royalblue',alpha=0.6)\n",
        "rects2 = ax.bar(ind+width, sorted_fox, width, color='red',alpha=0.6)\n",
        "rects3 = ax.bar(ind+2*width, sorted_bbc, width, color='seagreen',alpha=0.6)\n",
        "\n",
        "# add some\n",
        "ax.set_ylabel('Percentage of unreliable articles')\n",
        "ax.set_xlabel('Number of models')\n",
        "ax.set_title('Reliability of news articles')\n",
        "ax.set_xticks(ind + width /2)\n",
        "ax.set_xticklabels( ('0', '1', '2', '3', '4','5') )\n",
        "#ax.set_alpha(0.9)\n",
        "ax.legend( (rects1[0], rects2[0],rects3[0]), ('CNN', 'FOX','BBC') )\n",
        "\n",
        "ax1=fig.add_subplot(122)\n",
        "ax1.set_title('Percentage of articles found unreliable by at least 3 models')\n",
        "rects11 = ax1.bar([0, 1, 2], [np.sum(sorted_cnn[3:]),0,0], 0.7, color='royalblue',alpha=0.6)\n",
        "rects12 = ax1.bar([0, 1, 2], [0,np.sum(sorted_fox[3:]),0], 0.7, color='red',alpha=0.6)\n",
        "rects13 = ax1.bar([0, 1, 2], [0,0,np.sum(sorted_bbc[3:])], 0.7, color='seagreen',alpha=0.6)\n",
        "#ax1.legend( (rects11[0], rects12[0],rects13[0]), ('CNN', 'FOX','BBC') )\n",
        "ax1.set_xticks(np.array([0,1,2]))\n",
        "ax1.set_xticklabels( ('CNN', 'FOX', 'BBC') )\n",
        "ax1.set_xlabel('News source')\n",
        "ax1.set_ylabel('Percentage of unreliable articles')\n",
        "plt.show()\n",
        "plt.savefig(project_dir+'results.jpg',bbox_inches='tight')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ba6d3d70b007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_reliability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mfox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfox_reliability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbbc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbc_reliability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#plt.figure();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cnn_reliability' is not defined"
          ]
        }
      ]
    }
  ]
}